{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nathan Azoulay\n",
    "# Logistic Regression\n",
    "# CS581\n",
    "# Note: The assignment answers are all the way at the bottom starting at the bold header labeled assignment.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "# Using a corpus of movie review data\n",
    "# 2000 positive and negative reviews, evenly balanced.\n",
    "\n",
    "from nltk.corpus import movie_reviews as mr\n",
    "\n",
    "# If you want to read the corpus collectors' introduction to\n",
    "# this corpus, uncomment the next line.\n",
    "#print mr.readme()\n",
    "\n",
    "def add_data_from_files (file_list,data_list):\n",
    "    for f in file_list:\n",
    "        with open(f,'r') as fh:\n",
    "            data_list.append(fh.read())\n",
    "\n",
    "def get_data (clses, data_dirs, training_proportion = .9):\n",
    "    # We're going to compile 4 lists: training data and labels, test data and labels\n",
    "    train_data, train_labels = [], []\n",
    "    test_data, test_labels = [], []\n",
    "\n",
    "    for i,cls  in enumerate(clses):\n",
    "        d_dir = data_dirs[i]\n",
    "        os.chdir(d_dir)\n",
    "        cls_files = os.listdir(d_dir)\n",
    "        num_cls_files = len(cls_files)\n",
    "        training_index = int(training_proportion * num_cls_files)\n",
    "        train_labels.extend(cls for f in cls_files[:training_index])\n",
    "        test_labels.extend(cls for f in cls_files[training_index:])\n",
    "        add_data_from_files (cls_files[:training_index],train_data)\n",
    "        add_data_from_files (cls_files[training_index:],test_data)\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "data = dict(pos = mr.fileids('pos'),\n",
    "            neg = mr.fileids('neg'))\n",
    "\n",
    "def get_review_text (cls,file_id,start=0,end=None):\n",
    "    words = list(mr.words(data[cls][file_id]))\n",
    "    return ' '.join(words[start:end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating classifier output\n",
    "#Precision: percentage of true positives out all positive guesses the system made\n",
    "#Accuracy: percentage of correct answers out of total corpus\n",
    "#Recall: percentage of true positives out of all good reviews\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score,accuracy_score\n",
    "from collections import defaultdict\n",
    "\n",
    "flip = dict(pos = 'neg', neg = 'pos')\n",
    "test_dict = defaultdict(list)\n",
    "def do_evaluation (predicted, actual, pos_label='pos', verbose=True, \n",
    "                   test_dict= None, system_id = '',\n",
    "                   two_way = False, omit_accuracy = False, override = True):\n",
    "    #predicted, actual = zip(*pairs)\n",
    "    #nb_pairs = list(zip(nb_predicted_labels, test_labels))\n",
    "    (precision, recall,accuracy) = (precision_score(actual,predicted,pos_label=pos_label), \n",
    "                                    recall_score(actual,predicted,pos_label=pos_label),\n",
    "                                    accuracy_score(actual,predicted))\n",
    "    if verbose:\n",
    "        print_results(precision, recall, accuracy, pos_label, omit_accuracy=omit_accuracy)\n",
    "    if test_dict is not None:\n",
    "        if override:\n",
    "            test_dict[system_id] = []\n",
    "        test_dict[system_id].append((pos_label, (precision, recall, accuracy)))\n",
    "    if two_way:\n",
    "        return ((precision, recall,accuracy),\n",
    "                 do_evaluation (predicted, actual, pos_label=flip[pos_label], \n",
    "                                verbose=verbose, test_dict= test_dict, \n",
    "                                system_id = system_id,\n",
    "                                two_way = False, omit_accuracy = True,\n",
    "                                override = False))\n",
    "    else:\n",
    "        return (precision, recall, accuracy)\n",
    "\n",
    "def print_results (precision, recall, accuracy, pos_label, omit_accuracy = False):\n",
    "    banner =  f'P/R Evaluation with pos label = {pos_label}'\n",
    "    Precision, Recall, Accuracy = 'Precision', 'Recall', 'Accuracy'\n",
    "    print ()\n",
    "    if not omit_accuracy:\n",
    "           print (f'{Accuracy:10s} {accuracy:.1%}')\n",
    "           print()\n",
    "    print (banner)\n",
    "    print ('=' * len(banner))\n",
    "    print (f'{Precision:10s} {precision:.1%}')\n",
    "    print (f'{Recall:10s} {recall:.1%}')\n",
    "    \n",
    "def print_test_dict (test_dict):\n",
    "    for k in test_dict.keys():\n",
    "        line0 = f'{k[0]:<10} {k[1]:<22}'\n",
    "        indent = len(line0)\n",
    "        print(line0, end = ' ')\n",
    "        print(f'{test_dict[k][0][0]:<10}', end='')\n",
    "        print(f'{test_dict[k][0][1][0]:.3f}')\n",
    "        print(f'{\"\":<{indent}} {test_dict[k][1][0]:<10}', end='')\n",
    "        print(f'{test_dict[k][1][1][0]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "# Load and split data\n",
    "import sklearn\n",
    "module_name = sklearn.__name__\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "class_name = lr.__name__\n",
    "import os.path\n",
    "\n",
    "system_id = (module_name, class_name)\n",
    "home = os.getenv('HOME')  \n",
    "# This is where MY NLTK data is.  Yours should be in a similar place relative\n",
    "# to what your machine thinks is HOME.\n",
    "data_dir = os.path.join(home,'nltk_data/corpora/movie_reviews/')\n",
    "\n",
    "clses = ['pos','neg']\n",
    "\n",
    "#  The data is in the data_dir, sorted into subdirectories, one for each class.\n",
    "data_dirs = [os.path.join(data_dir,cls) for cls in clses]\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = get_data(clses, data_dirs)\n",
    "\n",
    "# Train and test classifier\n",
    "\n",
    "#  We use a somewhat more traditional feature weights, called TfidF weights\n",
    "#  max_df = 0.5; \"df\" is document frequency.  \n",
    "#  Omit any word that occurs in more than half of the training data documents\n",
    "#. Try higher values for max_df, reducing the number of features\n",
    "max_df = .5\n",
    "vectorizer = TfidfVectorizer(stop_words='english', use_idf=True,sublinear_tf=True, \n",
    "                             binary = False, max_df = max_df\n",
    "                             )\n",
    "#vectorizer = TfidfVectorizer()\n",
    "# Now with data set represented as a list of strings (one from each file),\n",
    "# extract the TfidF features\n",
    "train_features = vectorizer.fit_transform(train_data)\n",
    "\n",
    "#  We extract features from the test data using the same vectorizer\n",
    "#  trained on training data. The TFIDF feature model has been fit to \n",
    "#  (depends only on) the training data.\n",
    "test_features = vectorizer.transform(test_data)\n",
    "\n",
    "# Create an Logistic Regression Classifier instance\n",
    "# using default settings\n",
    "max_iter = 100. # The default\n",
    "solver = 'lbfgs' # The default, I think\n",
    "lr_clf = lr(solver = solver, max_iter = max_iter)\n",
    "\n",
    "# Train (or \"fit\") the model to the training data.\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "\n",
    "# Test the model on the test data.  Evaluation below.\n",
    "lr_predicted_labels = lr_clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.05421534])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifying individual examples\n",
    "vec1 = vectorizer.transform(['Inception is the greatest movie ever'])\n",
    "print(lr_clf.predict(vec1))\n",
    "\n",
    "#Under the hood\n",
    "w, b  = lr_clf.coef_, lr_clf.intercept_  \n",
    "# Usparsify, flatten, use the feature weights\n",
    "logit = (vec1.toarray().ravel().dot(w.T) + b)\n",
    "logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51355052]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "loss = expit(logit)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "\n",
      "logit (no bias):     -0.2793\n",
      "bias value:          -0.1654\n",
      "logit (w/bias):      -0.4446\n",
      "prob:                 0.3906\n",
      "\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "# Function classify with test example at bottom\n",
    "def classify (text,  clf, vectorizer, verbose = False):\n",
    "    \"\"\" \n",
    "    Text is a string.\n",
    "    \"\"\"\n",
    "    vec = vectorizer.transform([text])\n",
    "    w, b  = clf.coef_, clf.intercept_  \n",
    "    # Usparsify, flatten, use the feature weights/bias, make it a prob\n",
    "    loss = expit(vec.toarray().ravel().dot(w.T) + b)[0]\n",
    "    if verbose:\n",
    "        # vec . w^T (dot product)\n",
    "        logit_nb = vec.toarray().ravel().dot(w.T).ravel()[0]\n",
    "        #print('logit (no bias): ', logit_nb)\n",
    "        print(f'{\"logit (no bias):\":<20} {logit_nb: .4f}')\n",
    "        print(f\"{'bias value: ':<20} {b[0]:.4f}\")              \n",
    "        print(f\"{'logit (w/bias): ':<20} {logit_nb + b[0]: .4f}\")\n",
    "        print(f\"{'prob: ':<20} {loss: .4f}\")\n",
    "        print()\n",
    "    if loss > .5:\n",
    "       return 'pos'\n",
    "    else:\n",
    "       return 'neg'\n",
    "              \n",
    "text = \"I don't know how anyone could sit through Inception\"\n",
    "print(classify(text, lr_clf, vectorizer) +'\\n')\n",
    "              \n",
    "text = \"I don't know how anyone could sit through Inception\"\n",
    "print(classify(text, lr_clf, vectorizer, verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy   88.0%\n",
      "\n",
      "P/R Evaluation with pos label = pos\n",
      "===================================\n",
      "Precision  87.3%\n",
      "Recall     89.0%\n",
      "\n",
      "P/R Evaluation with pos label = neg\n",
      "===================================\n",
      "Precision  88.8%\n",
      "Recall     87.0%\n",
      "\n",
      "51.0% classifier predictions positive\n",
      "50.0% reviews positive\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier\n",
    "# Guessing 51% positive on a 50/50 dataset\n",
    "\n",
    "\n",
    "do_evaluation (lr_predicted_labels, test_labels, pos_label='pos', verbose=True, \n",
    "               system_id = system_id, \n",
    "               test_dict = test_dict, two_way = True)\n",
    "\n",
    "test_size = len(test_labels)\n",
    "pos_cnt = lambda lbls: len([cl for cl in lbls if cl == \"pos\"])\n",
    "\n",
    "print()\n",
    "print (f'{pos_cnt(lr_predicted_labels)/test_size:.1%} classifier predictions positive')\n",
    "\n",
    "#  Was the test data biased?  Were most of the test reviews positive, for example?\n",
    "print (f'{pos_cnt(test_labels)/test_size:.1%} reviews positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {('sklearn',\n",
       "              'LogisticRegression'): [('pos',\n",
       "               (0.8725490196078431, 0.89, 0.88)), ('neg',\n",
       "               (0.8877551020408163, 0.87, 0.88))]})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation results stored in dict\n",
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 37834), (1, 37834))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Features for the first document from our train data\n",
    "x0sparse = train_features[0]\n",
    "x0 = x0sparse.toarray()             #document vector = x, y is class assigned to it\n",
    "\n",
    "# model stores w and b\n",
    "w, b = lr_clf.coef_, lr_clf.intercept_\n",
    "\n",
    "#Both are 3D vectors\n",
    "x0.shape, w.shape\n",
    "\n",
    "#37834 is size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2805, 37406,  4210, 25235, 32828, 32459, 36679,  2705, 35378,\n",
       "        29502, ..., 24601, 33636, 26681, 24608, 34727, 15687, 11661,\n",
       "        24596, 14530, 19518]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort indices of weight vector from lowest valued index to highest\n",
    "max_indices = w.argsort()\n",
    "max_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'life'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dir(vectorizer)\n",
    "feats = vectorizer.get_feature_names()\n",
    "print(len(feats)) # Vocab size\n",
    "# Most highly weighted word!\n",
    "feats[19518]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  ['perfectly' 'terrific' 'quite' 'performance' 'true' 'hilarious'\n",
      " 'excellent' 'perfect' 'great' 'life']\n",
      "Min:  ['bad' 'worst' 'boring' 'plot' 'supposed' 'stupid' 'waste' 'awful'\n",
      " 'unfortunately' 'script']\n"
     ]
    }
   ],
   "source": [
    "# Lookup highest/lowest weight features\n",
    "\n",
    "n = 10\n",
    "# top_n_feats contains the feature indices of the n highest-weighted features\n",
    "# ravel flattens a 2D array into a 1D array (vector)\n",
    "top_n_feats = max_indices.ravel()[-n:]\n",
    "bottom_n_feats = max_indices.ravel()[:n]\n",
    "# name_array[i]  returns word with feature index i\n",
    "name_array = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "# threshold: number of items that triggers summarization of an array.  [default 1000 (?)]\n",
    "# edgeitems: num items to print from each end when summarizing [default 3]\n",
    "# print(np.set_printoptions.__doc__)  \n",
    "np.set_printoptions(threshold=20)\n",
    "print('Max: ', name_array[top_n_feats])\n",
    "print('Min: ', name_array[bottom_n_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap above into a function\n",
    "\n",
    "def find_max_min_feats (vec, vectorizer, n=10, positive_vals_only = False,\n",
    "                        verbose = False, prune = True, w = None,\n",
    "                        get_vals = False):\n",
    "    \"\"\"\n",
    "    Print features containing top n and bottom n values in a feature vector,\n",
    "    where feature names are defined by vectorizer.\n",
    "    \n",
    "    Use with weight vector:\n",
    "       >>> w = lr_clf.coef_\n",
    "       >>> find_max_min_feats (w, vectorizer, n=10)\n",
    "       ['perfectly' 'terrific' 'quite' 'performance' 'true' 'hilarious'\n",
    "        'excellent' 'perfect' 'great' 'life']\n",
    "       ['bad' 'worst' 'boring' 'plot' 'supposed' 'stupid' 'waste' 'awful'\n",
    "        'unfortunately' 'script']\n",
    "    \n",
    "    positive_vals_only should be set to true when the vector only has \n",
    "    positive values.  In that case, we assume we are still only interested in the \n",
    "    lowest nonzero values.\n",
    "    \"\"\"\n",
    "    vec = vec.ravel()\n",
    "    max_indices = vec.argsort()\n",
    "    top_n_feats = max_indices[-n:]\n",
    "    if verbose:\n",
    "        print('Max feats:')\n",
    "        print_feat_info (top_n_feats, vec, w, verbose = verbose)\n",
    "    if prune:\n",
    "        # restrict top_n to truly positive feats, even if less than n feats\n",
    "        top_n_feats = [f for f in top_n_feats if vec[f] > 0]\n",
    "    if positive_vals_only:\n",
    "       vec_cp = vec.copy()\n",
    "       # Assign the max value to all zero-valued positions\n",
    "       # removing 0-values from the competition for min value\n",
    "       vec_cp[vec==0] = vec[top_n_feats[-1]]\n",
    "       # Now find min valued feats\n",
    "       max_indices2 = vec_cp.argsort().ravel()\n",
    "       bottom_n_feats = max_indices2[:n]\n",
    "    else:\n",
    "        bottom_n_feats = max_indices[:n]\n",
    "        if prune:\n",
    "            bottom_n_feats = [f for f in bottom_n_feats if vec[f] < 0]\n",
    "    if verbose:\n",
    "       print()\n",
    "       print('Min feats:')\n",
    "       print_feat_info (bottom_n_feats, vec, w, verbose = verbose)\n",
    "       print()\n",
    "    name_array = np.array(vectorizer.get_feature_names())\n",
    "    # Reset numpy print options to print n feats. first get old settings\n",
    "    opts = np.get_printoptions()\n",
    "    # Reset.\n",
    "    np.set_printoptions(threshold=2 * n)\n",
    "    print('Max: ', name_array[top_n_feats])\n",
    "    print('Min: ', name_array[bottom_n_feats])\n",
    "    # Put old settings back\n",
    "    np.set_printoptions(**opts)\n",
    "    if get_vals:\n",
    "       return (top_n_feats, name_array[top_n_feats],\n",
    "               bottom_n_feats, name_array[bottum_n_feats])\n",
    "\n",
    "def print_feat_info (feat_index_vector, vec, w = None, verbose = False):\n",
    "    if (verbose >= 2): \n",
    "        print('Feat indices: ', feat_index_vector) \n",
    "    print('Feat values: ', [vec[f] for f in feat_index_vector])\n",
    "    if w is not None:\n",
    "       print('Feat weights: ', [w[f] for f in feat_index_vector])\n",
    "\n",
    "   \n",
    "def find_max_min_feats_classifier (clf, vectorizer, n = 10, verbose = False,\n",
    "                                      weight_attr = 'coef_'):\n",
    "    w = getattr(clf, weight_attr).ravel()\n",
    "    find_max_min_feats (w, vectorizer, n=n, verbose = verbose,positive_vals_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  ['perfectly' 'terrific' 'quite' 'performance' 'true' 'hilarious'\n",
      " 'excellent' 'perfect' 'great' 'life']\n",
      "Min:  ['bad' 'worst' 'boring' 'plot' 'supposed' 'stupid' 'waste' 'awful'\n",
      " 'unfortunately' 'script']\n"
     ]
    }
   ],
   "source": [
    "find_max_min_feats_classifier (lr_clf, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66501437])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy version of sigmoid is expit, applied to document vector x0\n",
    "x0sparse = train_features[0]\n",
    "x0 = x0sparse.toarray().ravel()\n",
    "loss = expit(x0.dot(w.T) + b)\n",
    "\n",
    "loss # probability that the sample belongs to the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'pos']\n",
      "[0.33498563 0.66501437]\n",
      "0.6650\n",
      "0.66501\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Returns probability of the sample for each class in the model, classes are ordered as they are in lr_clf.classes_\n",
    "print(lr_clf.classes_)\n",
    "prob_array = lr_clf.predict_proba(x0sparse).ravel()\n",
    "print(prob_array)\n",
    "# Too few decimal places: High prob looks liek 1\n",
    "print(f\"{prob_array[1]:.4f}\")\n",
    "# Enough decimal places.\n",
    "print(f\"{prob_array[1]:.5f}\")\n",
    "print(lr_clf.predict_proba(x0sparse).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos'], dtype='<U3')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict method chooses a class\n",
    "lr_clf.predict(x0sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68572103])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.dot(x0.T) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding active features in the document\n",
    "\n",
    "def find_max_min_feats_docvec (vec, clf, vectorizer, n = 10, prune = True,\n",
    "                               verbose = False, weight_attr = 'coef_',\n",
    "                               get_vals = False):\n",
    "    \"\"\"\n",
    "    Vec is a document vector.  Classifier a trained classifier.  Vectorizer\n",
    "    is the vectorizer that produced vec.\n",
    "    \n",
    "    Return highest- and lowest- weighted features per this trained classifier,\n",
    "    \"\"\"\n",
    "    w = getattr(clf, weight_attr).ravel()\n",
    "    dw = (vec.toarray().ravel() * w)\n",
    "    find_max_min_feats (dw, vectorizer, n=n, prune = prune, verbose = verbose, w = w)\n",
    "    if get_vals:\n",
    "       return dw\n",
    "    \n",
    "def find_max_min_feats_text (text, clf, vectorizer, n = 10, prune=True,\n",
    "                             verbose = False, weight_attr = 'coef_',\n",
    "                             get_vals = False):\n",
    "    \"\"\"\n",
    "    Text is a string.  Classifier a trained classifier.  Vectorizer\n",
    "    a trained vectorizer for turning strings into doc vectors.\n",
    "    \n",
    "    Return most and least siugnificant features per classifier.\n",
    "    \"\"\"\n",
    "    # Need to pass vectorizer a list of strings\n",
    "    vec = vectorizer.transform([text])\n",
    "    # Make sparse array an array,  flatten, get weighted feats [modified equation (a)]\n",
    "    # dw = (vec.toarray().ravel() * w) + b\n",
    "    dw = find_max_min_feats_docvec (vec, clf, vectorizer, n=10, prune = prune,\n",
    "                                    verbose = verbose, weight_attr = weight_attr)\n",
    "    if get_vals:\n",
    "        return vec, dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'films adapted from comic books have had plenty of success , whether they \\' re about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there \\' s never really been a comic book like from hell before . for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid \\' 80s with a 12 - part series called the watchmen . to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . the book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . in other words , don \\' t dismiss this film because of its source . if you can get past the whole comic book thing , you might find another stumbling block in from hell \\' s directors , albert and allen hughes . getting the hughes brothers to direct this seems almos'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the text of the first positive review\n",
    "get_review_text ('pos',0)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37834,)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec11 = vectorizer.transform([get_review_text ('pos',0)]).toarray().ravel()\n",
    "vec11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  ['hughes' 'hell' 'graham' 'depp' 'moore' 'campbell' 'ghetto' 'whitechapel'\n",
      " 'abberline' 'ripper']\n",
      "Min:  ['films' 'bad' 'scenes' 'new' 'great' 'better' 'love' 'big' 'long' 'isn']\n"
     ]
    }
   ],
   "source": [
    "find_max_min_feats (vec11, vectorizer, n=10, positive_vals_only = True) #weighting of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16435001314847014"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TFIDF value\n",
    "vec11.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 37834)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform document vector by applying weights to each feature -> weighted document vector\n",
    "w, b  = lr_clf.coef_, lr_clf.intercept_  \n",
    "dw = (vec11 * w)\n",
    "dw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias:  [-0.16537522] tfidf: 0.05359015649564609 weight: 0.4413422719369864\n",
      "logit with bias:  [-0.14172362] Negative value!\n",
      "Easier to understand logit w/o bias\n",
      "0.023651601421247094\n"
     ]
    }
   ],
   "source": [
    "# Constructed weighted document vectors out of logits without bias\n",
    "ind = vectorizer.vocabulary_['captures']\n",
    "\n",
    "print('bias: ', b, 'tfidf:', vec11[ind], 'weight:', w[0][ind])\n",
    "print('logit with bias: ',(vec11[ind] * w[0][ind]) + b, 'Negative value!')\n",
    "print('Easier to understand logit w/o bias')\n",
    "print((vec11[ind] * w[0][ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  ['job' 'captures' 'oscar' 'comic' 'great' 'campbell' 'era' 'graham'\n",
      " 'world' 'strong']\n",
      "Min:  ['bad' 'wasn' 'don' 'attempt' 'ludicrous' 'looks' 'batman' 'tries' 'blame'\n",
      " 'point']\n"
     ]
    }
   ],
   "source": [
    "# Find max/min features of dw\n",
    "# Finding the top features in vec\n",
    "w = lr_clf.coef_\n",
    "# sparse => array, flatten, compute weighted doc vector\n",
    "dw = (vec11 * w[0])\n",
    "# Same function that we use for finding max min feats in the classifier weights.\n",
    "find_max_min_feats (dw, vectorizer, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  ['job' 'captures' 'oscar' 'comic' 'great' 'campbell' 'era' 'graham'\n",
      " 'world' 'strong']\n",
      "Min:  ['bad' 'wasn' 'don' 'attempt' 'ludicrous' 'looks' 'batman' 'tries' 'blame'\n",
      " 'point']\n"
     ]
    }
   ],
   "source": [
    "# Wrap into function for future use\n",
    "find_max_min_feats_text (get_review_text ('pos',0), lr_clf, vectorizer, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  ['light' 'perfect' 'unlike' 'delightful' 'damon' 'great' 'job' 'wonderful'\n",
      " 'hilarious' 'oscar']\n",
      "Min:  ['script' 'boring' 'plot' 'supposed' 'dull' 'perry' 'dialogue'\n",
      " 'whatsoever' 'flat' 'material']\n"
     ]
    }
   ],
   "source": [
    "# Document vector, a sparse matrix\n",
    "d0sparse = train_features[0]\n",
    "# To make th evector suitablke for operations with w, b, we do this:\n",
    "# Unsparsify and flatten.  This will be done inside find_max_min_feats_docvec \n",
    "d0 = d0sparse.toarray().ravel()\n",
    "# Add in feat weights to get weighted doc vec\n",
    "dw = (d0 * w[0])\n",
    "find_max_min_feats (dw, vectorizer, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  ['light' 'perfect' 'unlike' 'delightful' 'damon' 'great' 'job' 'wonderful'\n",
      " 'hilarious' 'oscar']\n",
      "Min:  ['script' 'boring' 'plot' 'supposed' 'dull' 'perry' 'dialogue'\n",
      " 'whatsoever' 'flat' 'material']\n"
     ]
    }
   ],
   "source": [
    "# Wrapped into a function\n",
    "weighted_doc_vec = find_max_min_feats_docvec (d0sparse, lr_clf, vectorizer, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 37834)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifying individual examples with analysis\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    " ]\n",
    "vectorizer0 = TfidfVectorizer()\n",
    "X = vectorizer0.fit_transform(corpus)\n",
    "vectorizer.transform(['document first and']).toarray().ravel()\n",
    "\n",
    "vec11 = vectorizer.transform([get_review_text ('pos',0)])\n",
    "vec11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use function to classify made up examples\n",
    "text = 'Inception is the greatest movie ever'\n",
    "classify(text, lr_clf, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  ['inception' 'greatest']\n",
      "Min:  []\n"
     ]
    }
   ],
   "source": [
    "# This means two words carry positive weights as features and none carry negative\n",
    "# only inception and greatest have been learning in the training data\n",
    "find_max_min_feats_text (text, lr_clf, vectorizer, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg'], dtype='<U3')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "vec = vectorizer.transform(['Inception is the worst movie ever'])\n",
    "lr_clf.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "Max:  ['inception']\n",
      "Min:  ['worst']\n"
     ]
    }
   ],
   "source": [
    "# analysis \n",
    "text = 'Inception is the worst movie ever'\n",
    "print(classify(text, lr_clf, vectorizer, verbose = False))\n",
    "find_max_min_feats_text (text, lr_clf, vectorizer, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "Max:  ['inception']\n",
      "Min:  ['don' 'sit' 'know']\n"
     ]
    }
   ],
   "source": [
    "text = \"I don't know how anyone could sit through Inception\"\n",
    "print(classify(text, lr_clf, vectorizer))\n",
    "find_max_min_feats_text (text, lr_clf, vectorizer, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2621686592991448"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = vectorizer.vocabulary_['sit']\n",
    "w[0][ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit (no bias):      0.5136\n",
      "bias value:          -0.1654\n",
      "logit (w/bias):       0.3482\n",
      "prob:                 0.5862\n",
      "\n",
      "pos\n",
      "Max:  ['loved']\n",
      "Min:  []\n"
     ]
    }
   ],
   "source": [
    "# Word repetition\n",
    "# Look at both examples and how it makes no difference\n",
    "\n",
    "text = \"I loved this movie\"\n",
    "print(classify(text, lr_clf, vectorizer, verbose = True))\n",
    "find_max_min_feats_text (text, lr_clf, vectorizer, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit (no bias):      0.5136\n",
      "bias value:          -0.1654\n",
      "logit (w/bias):       0.3482\n",
      "prob:                 0.5862\n",
      "\n",
      "pos\n",
      "Max:  ['loved']\n",
      "Min:  []\n"
     ]
    }
   ],
   "source": [
    "text = \"I loved loved loved this movie\"\n",
    "print(classify(text, lr_clf, vectorizer, verbose = True))\n",
    "find_max_min_feats_text (text, lr_clf, vectorizer, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "word = 'loved'\n",
    "vec12 = vectorizer.transform([f\"I {word} this movie\"])\n",
    "ind = vectorizer.vocabulary_[word]\n",
    "# Get the feature value for this example\n",
    "print(vec12.toarray().ravel()[ind])\n",
    "vec122 = vectorizer.transform([f\"I {word} {word} {word} this movie\"])\n",
    "# Get the feature value for the other example\n",
    "print(vec122.toarray().ravel()[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit (no bias):      0.0000\n",
      "bias value:          -0.1654\n",
      "logit (w/bias):      -0.1654\n",
      "prob:                 0.4588\n",
      "\n",
      "neg\n",
      "Max:  []\n",
      "Min:  []\n"
     ]
    }
   ],
   "source": [
    "# example with no known words\n",
    "text = 'xnwnei'\n",
    "print(classify(text, lr_clf, vectorizer, verbose = True))\n",
    "find_max_min_feats_text (text, lr_clf, vectorizer, n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy   70.5%\n",
      "\n",
      "P/R Evaluation with pos label = pos\n",
      "===================================\n",
      "Precision  68.5%\n",
      "Recall     76.0%\n",
      "\n",
      "P/R Evaluation with pos label = neg\n",
      "===================================\n",
      "Precision  73.0%\n",
      "Recall     65.0%\n",
      "\n",
      "55.5% classifier predictions positive\n",
      "50.0% reviews positive\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "import sklearn\n",
    "module_name = sklearn.__name__\n",
    "from sklearn.naive_bayes import MultinomialNB as nb\n",
    "class_name = nb.__name__\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if class_name == 'BernoulliNB':\n",
    "    binary = True\n",
    "elif class_name == 'MultinomialNB':\n",
    "    binary = False\n",
    "else:\n",
    "    raise Exception('binary var unset. Do you want count vectorization?  Is it binary?')\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english', max_df = .5, binary = binary)\n",
    "# Now with data set represented as a list of strings (one from each file),\n",
    "# extract the TFIDF features\n",
    "train_features = count_vectorizer.fit_transform(train_data)\n",
    "\n",
    "#  We extract features from the test data using the same vectorizer\n",
    "#  trained on training data. The TFIDF feature model has been fit to \n",
    "#  (depends only on) the training data.\n",
    "test_features = count_vectorizer.transform(test_data)\n",
    "\n",
    "# Turning smoothing off! [alpha REALLY close to 0]\n",
    "#No smoothing alpha\n",
    "no_smoothing_alpha=1.0e-10\n",
    "nb_clf = nb(alpha=no_smoothing_alpha)\n",
    "#nb_clf = MultinomialNB(alpha=no_smoothing_alpha)\n",
    "# Train (or \"fit\") the model to the training data.\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "# Test the model on the test data.\n",
    "nb_predicted_labels = nb_clf.predict(test_features)\n",
    "\n",
    "# Evaluate the results\n",
    "nb_pos_predictions = [p for p in nb_predicted_labels if p=='pos']\n",
    "\n",
    "\n",
    "\n",
    "do_evaluation (nb_predicted_labels, test_labels, pos_label='pos', verbose=True, \n",
    "               system_id = (module_name, class_name), \n",
    "               test_dict = test_dict, two_way = True)\n",
    "\n",
    "print()\n",
    "\n",
    "print (f'{pos_cnt(nb_predicted_labels)/test_size:.1%} classifier predictions positive')\n",
    "\n",
    "#  Was the test data biased?  Were most of the test reviews positive, for example?\n",
    "print (f'{pos_cnt(test_labels)/test_size:.1%} reviews positive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute new weight vector\n",
    "weight_attr = 'weights'\n",
    "\n",
    "def add_nb_w (clf, pos_cls = 'pos', weight_attr = 'weights'):\n",
    "    clses = clf.classes_\n",
    "    pos_index = list(clses).index(pos_cls)\n",
    "    nb_w_cl_pos = clf.feature_log_prob_[pos_index]\n",
    "    nb_w_cl_neg = clf.feature_log_prob_[(1 - pos_index)]\n",
    "    # pos_prob/neg_prob ratio > 1 means neg_log_prob/pos_log_prog ratio > 1,\n",
    "    weight_vector = np.log(nb_w_cl_neg/nb_w_cl_pos)\n",
    "    # Item assignment not implemented for sklearn learner instances\n",
    "    #nb_clf['weights'] = compute_nb_w (nb_clf)\n",
    "    setattr(clf, weight_attr, weight_vector)\n",
    "\n",
    "add_nb_w (nb_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "Max feats:\n",
      "Feat values:  [-0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 1.0372699332507003]\n",
      "Feat weights:  [-1.050457698954473, 0.03248696950987699, 0.1011008517948238, 0.0816721411795122, -0.010692368113133378, -1.050457698954473, 0.23025058518467753, 1.0372699332507003, 0.03659535262165882, 1.0372699332507003]\n",
      "\n",
      "Min feats:\n",
      "Feat values:  [-0.05553717257391996, -0.05455432724682377, -0.034708923222847625]\n",
      "Feat weights:  [-0.05553717257391996, -0.05455432724682377, -0.034708923222847625]\n",
      "\n",
      "Max:  ['inception']\n",
      "Min:  ['sit' 'don' 'know']\n"
     ]
    }
   ],
   "source": [
    "text = \"I don't know how anyone could sit through Inception\"\n",
    "print(classify(text, nb_clf, count_vectorizer))\n",
    "find_max_min_feats_text (text, nb_clf, count_vectorizer, n = 10, verbose = True,\n",
    "                          weight_attr = weight_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  ['griffiths' 'stephens' 'lithgow' 'skinhead' 'muriel' 'valjean' 'soviet'\n",
      " 'kurtz' 'chocolat' 'tibbs' 'slade' 'boone' 'reza' 'humbert' 'fantasia'\n",
      " 'coens' 'scarlett' 'booker' 'motta' 'bianca' 'pollock' 'capone'\n",
      " 'commodus' 'shyamalan' 'dolores' 'bubby' 'kaufman' 'giles' 'burbank'\n",
      " 'lumumba' 'rounders' 'niccol' 'matilda' 'sethe' 'farquaad' 'apostle'\n",
      " 'donkey' 'fei' 'guido' 'maximus' 'taran' 'mallory' 'lambeau' 'jude'\n",
      " 'sweetback' 'leila' 'argento' 'gattaca' 'ordell' 'shrek']\n",
      "Min:  ['jolie' 'brenner' 'sphere' 'palmetto' 'macdonald' 'pokemon' 'supergirl'\n",
      " 'jericho' 'jill' 'cisco' 'bilko' 'mona' 'hush' 'silverman' 'bruckheimer'\n",
      " 'memphis' 'kersey' 'mandingo' 'compensate' 'geronimo' 'psychlos' 'liu'\n",
      " 'grinch' 'highlander' 'hawk' 'elwood' 'degenerates' 'caulder' 'terl'\n",
      " 'vikings' 'rimbaud' 'angelina' 'wrestlers' 'dwayne' 'alessa' 'autistic'\n",
      " 'kip' 'horrid' 'diedre' 'diaries' 'zach' 'fern' 'loveless' 'brooke'\n",
      " 'switchback' 'peet' 'wayans' 'muresan' 'tatopoulos' 'forsythe']\n"
     ]
    }
   ],
   "source": [
    "find_max_min_feats_classifier(nb_clf, count_vectorizer, n = 50, weight_attr= weight_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos'], dtype='<U3')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec25 = count_vectorizer.transform(['Inception is the worst movie ever'])\n",
    "nb_clf.predict(vec25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pos'], dtype='<U3')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec26 = count_vectorizer.transform(['Inception is the best movie ever'])\n",
    "nb_clf.predict(vec26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Unweighted vector\n",
    "avec25 = vec25.toarray().ravel()\n",
    "opts = np.get_printoptions()\n",
    "np.set_printoptions(threshold=20, edgeitems = 10)\n",
    "print(avec25[:25])\n",
    "#np.set_printoptions(**opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 2)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For MultinomialNB the doc vector has count:\n",
    "avec25.max(),avec25.min(),avec25.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34860705, 0.65139295]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test repetition\n",
    "vec28 = count_vectorizer.transform(['I loved this movie'])\n",
    "nb_clf.predict_proba(vec28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13290625, 0.86709375]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec28 = count_vectorizer.transform(['I loved loved loved this movie'])\n",
    "nb_clf.predict_proba(vec28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  ['inception']\n",
      "Min:  ['worst']\n"
     ]
    }
   ],
   "source": [
    "# Finding max/min feats and the weighted doc vector\n",
    "wd = find_max_min_feats_docvec (vec25, nb_clf, count_vectorizer, \n",
    "                                n = 10, weight_attr = weight_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'pos']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-8.745478284939654,\n",
       " -8.745478284939654,\n",
       " -6.979802305330207,\n",
       " 0.7981041262603019)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intercept = class prior, as expected, reflecting the fact that our data is balanced.\n",
    "ind = count_vectorizer.vocabulary_['worst']\n",
    "classes = nb_clf.classes_\n",
    "nb_w = nb_clf.coef_.ravel()\n",
    "print(classes)\n",
    "nb_w_cl0 = nb_clf.feature_log_prob_[0]\n",
    "nb_w_cl1 = nb_clf.feature_log_prob_[1]\n",
    "# log p(greatest | pos), feature weight for greatest, log p(greatest|neg),\n",
    "# ratio: log p(greatest | pos)/ log p(greatest | neg)\n",
    "nb_w_cl1[ind], nb_w[ind], nb_w_cl0[ind], nb_w_cl0[ind]/nb_w_cl1[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6931471805599454 -0.6931471805599454\n",
      "0.49999999999999994\n"
     ]
    }
   ],
   "source": [
    "# negative feature\n",
    "x = nb_clf.class_log_prior_[1:][0]\n",
    "print(x,nb_clf.intercept_[0])\n",
    "print(np.exp(nb_clf.intercept_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-44.15016134, -23.01278725]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.fixes import logsumexp\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "\n",
    "x = count_vectorizer.transform(['Inception is the worst movie ever'])\n",
    "\n",
    "# th jll computation\n",
    "self = nb_clf\n",
    "# This could be precomputed. Saving space?  Not much.\n",
    "neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
    "#Compute  neg_prob · (1 - X).T  as  ∑neg_prob - X · neg_prob\n",
    "jll = safe_sparse_dot(x, (self.feature_log_prob_ - neg_prob).T)\n",
    "jll += self.class_log_prior_ + neg_prob.sum(axis=1)\n",
    "jll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.11373741e+01, -6.60929089e-10]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing\n",
    "log_prob_x = logsumexp(jll, axis=1)\n",
    "jll - np.atleast_2d(log_prob_x).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 37834)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.60929395e-10, 9.99999999e-01]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(jll - np.atleast_2d(log_prob_x).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.8325814637483102, -0.6931471805599453, 2.643856189774725)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of positive feature\n",
    "neg_prob, pos_prob = .16,.5\n",
    "np.log(neg_prob),np.log(pos_prob), np.log(neg_prob)/np.log(pos_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'pos']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-8.745478284939654, -6.979802305330207, 0.7981041262603019)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check our model\n",
    "ind = vectorizer.vocabulary_['worst']\n",
    "print(classes)\n",
    "nb_w_cl0 = nb_clf.feature_log_prob_[0]\n",
    "nb_w_cl1 = nb_clf.feature_log_prob_[1]\n",
    "# log p(greatest | pos), log p(greatest|neg),\n",
    "# ratio: log p(greatest | pos)/ log p(greatest | neg)\n",
    "nb_w_cl1[ind], nb_w_cl0[ind], nb_w_cl0[ind]/nb_w_cl1[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg' 'pos']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-12.574119681330924, -35.47792390894256, 2.8215035969172)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same thing for inception\n",
    "ind = vectorizer.vocabulary_['inception']\n",
    "nb_w = nb_clf.coef_\n",
    "classes = nb_clf.classes_\n",
    "(cl1, cl2) = classes\n",
    "print(classes)\n",
    "# cl2 is positive.  Note that feature_log_prob\n",
    "# for that class provides the weight vector.\n",
    "nb_w_cl0 = nb_clf.feature_log_prob_[0]\n",
    "nb_w_cl1 = nb_clf.feature_log_prob_[1]\n",
    "nb_w_cl1[ind], nb_w_cl0[ind], nb_w_cl0[ind]/nb_w_cl1[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'character',\n",
       " 'film',\n",
       " 'good',\n",
       " 'just',\n",
       " 'like',\n",
       " 'make',\n",
       " 'movie',\n",
       " 'story',\n",
       " 'time',\n",
       " 'way'}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a) The evaluation is after the next block of code where I printed test_dict.\n",
    "1b) The Logistic Regression shows the highest percentage of accuracy at 88% following with the Bernoulli that has an accuracy of 75.5%, and lastly the MultinomialNB with 70.5% accuracy. Between the BernoulliNB and MultinomialNB, the system with the highest percentage of the true positive reviews would be the BernoulliNB with a precision of 75.8% compared to the MultinomialNB with 68.5%. Below is the work for Bernoulli followed by the test_dict to compare the evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy   75.5%\n",
      "\n",
      "P/R Evaluation with pos label = pos\n",
      "===================================\n",
      "Precision  75.8%\n",
      "Recall     75.0%\n",
      "\n",
      "P/R Evaluation with pos label = neg\n",
      "===================================\n",
      "Precision  75.2%\n",
      "Recall     76.0%\n",
      "\n",
      "49.5% classifier predictions positive\n",
      "50.0% reviews positive\n"
     ]
    }
   ],
   "source": [
    "# BernoulliNB\n",
    "import sklearn\n",
    "module_name = sklearn.__name__\n",
    "from sklearn.naive_bayes import BernoulliNB as nb\n",
    "class_name = nb.__name__\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if class_name == 'BernoulliNB':\n",
    "    binary = True\n",
    "elif class_name == 'MultinomialNB':\n",
    "    binary = False\n",
    "else:\n",
    "    raise Exception('binary var unset. Do you want count vectorization?  Is it binary?')\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english', max_df = .5, binary = binary)\n",
    "# Now with data set represented as a list of strings (one from each file),\n",
    "# extract the TFIDF features\n",
    "train_features = count_vectorizer.fit_transform(train_data)\n",
    "\n",
    "#  We extract features from the test data using the same vectorizer\n",
    "#  trained on training data. The TFIDF feature model has been fit to \n",
    "#  (depends only on) the training data.\n",
    "test_features = count_vectorizer.transform(test_data)\n",
    "\n",
    "# Turning smoothing off! [alpha REALLY close to 0]\n",
    "#No smoothing alpha\n",
    "no_smoothing_alpha=1.0e-10\n",
    "nb_clf = nb(alpha=no_smoothing_alpha)\n",
    "#nb_clf = MultinomialNB(alpha=no_smoothing_alpha)\n",
    "# Train (or \"fit\") the model to the training data.\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "\n",
    "# Test the model on the test data.\n",
    "nb_predicted_labels = nb_clf.predict(test_features)\n",
    "\n",
    "# Evaluate the results\n",
    "nb_pos_predictions = [p for p in nb_predicted_labels if p=='pos']\n",
    "\n",
    "\n",
    "\n",
    "do_evaluation (nb_predicted_labels, test_labels, pos_label='pos', verbose=True, \n",
    "               system_id = (module_name, class_name), \n",
    "               test_dict = test_dict, two_way = True)\n",
    "\n",
    "print()\n",
    "\n",
    "print (f'{pos_cnt(nb_predicted_labels)/test_size:.1%} classifier predictions positive')\n",
    "\n",
    "#  Was the test data biased?  Were most of the test reviews positive, for example?\n",
    "print (f'{pos_cnt(test_labels)/test_size:.1%} reviews positive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {('sklearn',\n",
       "              'LogisticRegression'): [('pos',\n",
       "               (0.8725490196078431, 0.89, 0.88)), ('neg',\n",
       "               (0.8877551020408163, 0.87, 0.88))],\n",
       "             ('sklearn',\n",
       "              'MultinomialNB'): [('pos',\n",
       "               (0.6846846846846847, 0.76, 0.705)), ('neg', (0.7303370786516854,\n",
       "                0.65,\n",
       "                0.705))],\n",
       "             ('sklearn',\n",
       "              'BernoulliNB'): [('pos',\n",
       "               (0.7575757575757576, 0.75, 0.755)), ('neg', (0.7524752475247525,\n",
       "                0.76,\n",
       "                0.755))]})"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER FOR 1A\n",
    "test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) Below I have ran the sample text with both the Logistic Regression and Bernoulli classifier.\n",
    "The logistic regression outputs positive, while Bernoulli outputs negative review. In the logistic regression,one of the positive words is \"disgraced\", which is considered negative in the BernoulliNB. Some words such as redeemable and love should show up as positive words, so I am confused if I did my calculations correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit (no bias):      0.4067\n",
      "bias value:          -0.1654\n",
      "logit (w/bias):       0.2413\n",
      "prob:                 0.5600\n",
      "\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "text = 'Harrison Ford turns in a glowing performance as the disgraced but eminently redeemable John Thornton. As Buck comes to love Thornton, so do we'\n",
    "print(classify(text,lr_clf,vectorizer, verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit (no bias):     -39.6649\n",
      "bias value:          -0.6931\n",
      "logit (w/bias):      -40.3581\n",
      "prob:                 0.0000\n",
      "\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli\n",
    "text = 'Harrison Ford turns in a glowing performance as the disgraced but eminently redeemable John Thornton. As Buck comes to love Thornton, so do we'\n",
    "print(classify(text, nb_clf, count_vectorizer,verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  ['eminently' 'disgraced' 'turns' 'thornton' 'john' 'glowing' 'love'\n",
      " 'harrison' 'ford' 'performance']\n",
      "Min:  ['buck' 'comes']\n"
     ]
    }
   ],
   "source": [
    "# Logictic regression min and max\n",
    "find_max_min_feats_text(text,lr_clf,vectorizer,n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  []\n",
      "Min:  ['disgraced' 'eminently' 'thornton' 'buck' 'glowing' 'harrison' 'ford'\n",
      " 'turns' 'john' 'comes']\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli min and max\n",
    "find_max_min_feats_text(text, nb_clf, count_vectorizer,n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3) The repetition of a known positive word in a review does not affec the values for all three models. My speculation would be that this feature is built into each model to not affect the probabilities or numbers due to the repetition of a word. For the MultinomialNB I used the data used above since variable names are reused for BernoulliNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit (no bias):     -9.5245\n",
      "bias value:          -0.6931\n",
      "logit (w/bias):      -10.2177\n",
      "prob:                 0.0000\n",
      "\n",
      "neg\n",
      "\n",
      "logit (no bias):     -9.5245\n",
      "bias value:          -0.6931\n",
      "logit (w/bias):      -10.2177\n",
      "prob:                 0.0000\n",
      "\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli without repetition\n",
    "textb = 'My friend enjoyed the movie alot'\n",
    "print(classify(textb, nb_clf, count_vectorizer,verbose = True)+'\\n')\n",
    "\n",
    "# Bernoulli repetition\n",
    "textb = 'My friend enjoyed enjoyed the movie alot'\n",
    "print(classify(textb, nb_clf, count_vectorizer,verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit (no bias):      0.4067\n",
      "bias value:          -0.1654\n",
      "logit (w/bias):       0.2413\n",
      "prob:                 0.5600\n",
      "\n",
      "pos\n",
      "\n",
      "logit (no bias):      0.4067\n",
      "bias value:          -0.1654\n",
      "logit (w/bias):       0.2413\n",
      "prob:                 0.5600\n",
      "\n",
      "pos\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression without repetition\n",
    "textb = 'My friend enjoyed the movie alot'\n",
    "print(classify(text,lr_clf,vectorizer, verbose = True) +'\\n')\n",
    "\n",
    "# With repetition\n",
    "textb = 'My friend enjoyed enjoyed the movie alot'\n",
    "print(classify(text,lr_clf,vectorizer, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4) A document vector is a vector corresponding to x in the equation sigmoid(dot(wx) + b). However, a weighted document vector is present in this equation because of the w which is a weight vector for our features. The value can be positive or negative, the higher means important features. On the other hand, values closer to 0 define features that don't decide or do much. In the formula (a) x should be the document vector because it contains the example to be classified in which y defines the class that will be assigned to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5) Stop words are words like \"and\",\"the\",\"him\", which are uninformative in representing content of a text, and can be removed to avoid having them conflict with our probabilities. As of now 'english' is the only supported string value. The words we have in stop words are \"movie\", \"character\", \"make\",\"story\",\"time\". These are all words that do not have any positive or negative context because they are for the most part nouns that do not reflect any positive or negative bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6) Suppose we are using a logistic regression model. w = (-.1, 1.2, -.6) b = -1.3\n",
    "Determine if vectors are classified positive or negative\n",
    "\n",
    "a) (0,0,-.3)\n",
    "    Based off a sigmoid functions attributes, I can tell it will be negative because we are subtracting a small number by a big negative number, resulting in a negative number, so we can assume it will be below .5\n",
    "    \n",
    "b) (2,-.4,13)\n",
    "    This vector as well will result in a large negative number, even larger than the one above, meaning it will fall below .5 most likely, causing it to be negative class.\n",
    "    \n",
    "c) (5,2,3)\n",
    "    This vector I believe would result in a positive classification because of the large positive number. (My logic was incorrect here when looking at the probability calculated below after making an educated guess.\n",
    "\n",
    "Part B\n",
    "Compute P(y=1) for each of the 3 above vectors\n",
    "Used python below to use sigmoid function because I couldn't get dot product to work so calculated that part by hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First vector p:  0.2460112835510519\n",
      "Second vector p:  5.656859586026619e-05\n",
      "Third vector p:  0.23147521650098238\n"
     ]
    }
   ],
   "source": [
    "# First Vector\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "# Used a calculator to get number plugged into expit which is the dot product of (w,x) + b\n",
    "# I kept getting errors when trying to use dot function\n",
    "\n",
    "a = expit(-1.12)\n",
    "print(\"First vector p: \",a)\n",
    "\n",
    "b = expit(-9.78)\n",
    "print(\"Second vector p: \",b)\n",
    "\n",
    "c = expit(-1.2)\n",
    "print(\"Third vector p: \",c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7) When raising the value of max_df, the vocabulary size increased by 10 when increasing the df to .9\n",
    "When lowering the value of max_df, the vocabulary size decreased significantly by over 400 when lowering the df to .1. My speculation would be that because the df descreased, a smaller vocabulary size will be present to be considered when comparing to our df value. Same way with a higher df, more words will be considered in comparison to our df value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
